{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1695185,"sourceType":"datasetVersion","datasetId":1004665}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\n\n# Define paths\ndata_dir = '/kaggle/input/rice-plant-dataset/Rice Seed Dataset'\nhealthy_dir = os.path.join(data_dir, 'Healthy_Images')\nunhealthy_dir = os.path.join(data_dir, 'Unhealthy_Images')\n\n# List all image files\nhealthy_files = [os.path.join(healthy_dir, f) for f in os.listdir(healthy_dir) if f.endswith('.jpg')]\nunhealthy_files = [os.path.join(unhealthy_dir, f) for f in os.listdir(unhealthy_dir) if f.endswith('.jpg')]\n\n# Create labels\nhealthy_labels = [0] * len(healthy_files)\nunhealthy_labels = [1] * len(unhealthy_files)\n\n# Combine files and labels\nfiles = healthy_files + unhealthy_files\nlabels = healthy_labels + unhealthy_labels\n\n# Split data into training and validation sets\ntrain_files, val_files, train_labels, val_labels = train_test_split(files, labels, test_size=0.2, random_state=42)\n\n# Define custom dataset\nclass CustomDataset(Dataset):\n    def __init__(self, files, labels, transform=None):\n        self.files = files\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        img_path = self.files[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Define transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Create custom datasets\ntrain_dataset = CustomDataset(train_files, train_labels, transform=transform)\nval_dataset = CustomDataset(val_files, val_labels, transform=transform)\n\n# Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Load AlexNet model\nmodel = models.alexnet(pretrained=True)\nnum_features = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(num_features, 2)  # 2 classes: healthy and unhealthy\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 5\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Print training loss\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}')\n\n    # Validate the model\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    # Print validation accuracy\n    print(f'Validation Accuracy: {correct/total:.4f}')\n\nprint('Training complete!')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T08:29:41.432641Z","iopub.execute_input":"2024-02-28T08:29:41.433112Z","iopub.status.idle":"2024-02-28T08:41:27.339733Z","shell.execute_reply.started":"2024-02-28T08:29:41.433071Z","shell.execute_reply":"2024-02-28T08:41:27.338371Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 124MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Loss: 0.9127\nValidation Accuracy: 0.5000\nEpoch 2/5, Loss: 0.7000\nValidation Accuracy: 0.5000\nEpoch 3/5, Loss: 0.6967\nValidation Accuracy: 0.5000\nEpoch 4/5, Loss: 0.6927\nValidation Accuracy: 0.5000\nEpoch 5/5, Loss: 0.6949\nValidation Accuracy: 0.5000\nTraining complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()  # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f'Accuracy on validation set: {accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T08:48:57.182875Z","iopub.execute_input":"2024-02-28T08:48:57.183332Z","iopub.status.idle":"2024-02-28T08:49:15.651220Z","shell.execute_reply.started":"2024-02-28T08:48:57.183299Z","shell.execute_reply":"2024-02-28T08:49:15.650167Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy on validation set: 0.5000\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = transform(image).unsqueeze(0).to(device)\n    model.eval()\n    with torch.no_grad():\n        output = model(image_tensor)\n        _, predicted = torch.max(output, 1)\n        return predicted.item()\n\n# Example usage\nimage_path = '/kaggle/input/rice-plant-dataset/Rice Seed Dataset/Healthy_Images/healthy (1).jpg'\nprediction = predict_image(image_path)\nif prediction == 0:\n    print('The rice plant is healthy.')\nelse:\n    print('The rice plant is unhealthy.')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T08:50:59.151135Z","iopub.execute_input":"2024-02-28T08:50:59.151674Z","iopub.status.idle":"2024-02-28T08:50:59.271709Z","shell.execute_reply.started":"2024-02-28T08:50:59.151637Z","shell.execute_reply":"2024-02-28T08:50:59.270443Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The rice plant is healthy.\n","output_type":"stream"}]}]}